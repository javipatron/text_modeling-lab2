---
title: "Lab 2"
author: "Javier Patr√≥n"
date: "r Sys.Date()"
output: html_document
---

# Assignment (Due 4/18 by 11:59 PM)
#### Lab 2
Sentiment analysis is a tool for assessing the mood of a piece of text. For example, we can use sentiment analysis to understand public perceptions of topics in environmental policy like energy, climate, and conservation.

### 1.  Access the Nexis Uni database through the UCSB library: <https://www.library.ucsb.edu/research/db/211

### 2.  Choose a key search term or terms to define a set of articles.

In this lab, I will be exploring the words 'blue cabron,' and 'mangrove.' I am particularly interested in investigating the relationship between positivism and negativism as they relate to political and government initiatives in the environment.

[Here](https://search.library.ucsb.edu/discovery/search?query=any,contains,blue%20carbon&tab=Everything&search_scope=DN_and_CI&vid=01UCSB_INST:UCSB&lang=en&offset=0) is a link that takes you to the related articles.

### 3.  Use your search term along with appropriate filters to obtain and download a batch of at least 100 full text search results (.docx).

-   Guidance for {LexisNexisTools} : <https://github.com/JBGruber/LexisNexisTools/wiki/Downloading-Files-From-Nexis>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(LexisNexisTools)
library(dplyr)
library(readr)
library(stringr)
library(here)
library(tidytext)
library(tidyr)
library(ggplot2)
```


### 4.  Read your Nexis article document into RStudio.
```{r}

# Set my working directory
setwd(here("lab2_files"))

# Get the 148 articles that I previously downloaded from the UCSB Library Portal using the Nexis Uni Tool
my_files <- list.files(pattern = ".docx", 
                       path = getwd(),
                       full.names = TRUE, 
                       recursive = TRUE, 
                       ignore.case = TRUE)

text_dat <- lnt_read(my_files)

# We'll use the {LexisNexisTools} package to handle the documents from our Nexis search.
#dat <- readRDS(here("lexisdat.RDS"))

```


### 5.  This time use the full text of the articles for the analysis. First clean any artifacts of the data collection process (hint: this type of thing should be removed: "Apr 04, 2022 (Biofuels Digest: <http://www.biofuelsdigest.com/> Delivered by Newstex"))

### INCOMPLETE

6.  Explore your data a bit and replicate the analyses above presented in class (Lab2)

```{r}
# The @ is for indexing within tibbles
meta_df <- text_dat@meta
articles_df <- text_dat@articles
paragraphs_df <- text_dat@paragraphs

dat2 <- tibble(Date = meta_df$Date, 
              Headline = meta_df$Headline, 
              id = text_dat@articles$ID, 
              text = text_dat@articles$Article)

```

```{r}
# create a list of all the words in the dictionary and if they have a sentiment word attached like "positive" or "negative".

bing_sent <- get_sentiments('bing') #grab the bing sentiment lexicon from tidytext

```

Score words using bing lexicon
```{r text_words}
# Create a new data frame that has a row for each word
text_words <- dat2 |> 
  unnest_tokens(output = word, 
                input = text,
                token = 'words')

# Create a new data frame that has 
sentiment_words <- text_words |> 
  anti_join(stop_words) |> # Removing the unuseful words
  inner_join(bing_sent)  |> # Adding the sentiment related column
  mutate(sent_num = case_when(sentiment == "negative" ~ -1,
                              sentiment == "positive" ~ 1))

mean(sentiment_words$sent_num)


```

Calculate mean sentiment (by word polarity) across articles
```{r mean_sent}
sentiment_article <- sentiment_words |> 
  group_by(Headline) |> 
  count(id, sentiment) |> 
  pivot_wider(names_from = sentiment, 
              values_from = n,
              values_fill = 0) |> 
  mutate(polarity = positive - negative)

mean(sentiment_article$polarity)

```

Sentiment by article plot

Let's try a very basic plot to see the amount of positive vs. negative articles.
```{r plot_sent_scores}

ggplot(sentiment_article, aes(x = id)) +
  theme_classic() +
  geom_col(aes(y = positive), stat = "identity", fill = "slateblue4") +
  geom_col(aes(y = negative), stat = "identity", fill = "red") +
  theme(axis.title.y = element_blank()) +
  labs(title = "Sentiment Analysis",
       subtitle = "Blue Carbon/ Mangrove", 
       y = "Sentiment score",
       x = "ID of the Article")

```


7.  Look at the nrc contribution to emotion by word plots. Identify and reclassify or remove at least one term that gives misleading results in your context.

```{r nrc_sentiment}
# Create a new data frame with the sentiments per word.
word_sentiment <- get_sentiments("nrc") |> 
  filter(word != "blue")

# Create a new data frame that has the text_words with the column of nrc sentiment
nrc_word_count <- text_words |> 
  anti_join(stop_words, by= "word") |> 
  inner_join(word_sentiment) |> 
  count(word, sentiment, sort = T)


```

8.  Plot the amount of nrc emotion words as a percentage of all the emotion words used each day (aggregate text from articles published on the same day). How does the distribution of emotion words change over time? Can you think of any reason this would be the case?

```{r sent_counts}
sentiment_group_counts <- text_words |> 
  anti_join(stop_words, by = "word") |> 
  group_by(id) |> 
  inner_join(word_sentiment) |> 
  group_by(sentiment) |> 
  count(word, sentiment, sort = T)

sentiment_group_counts |> 
  group_by(sentiment) |> 
  slice_max(n,n= 5) |> 
  ungroup() |> 
  mutate(word = reorder(word,n)) |> 
  ggplot(aes(n, word,fill = sentiment)) +
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x= "Contribution to Sentiment", 
       y = NULL)
  
```

## INCOMPLETE

# Plot all the emotion word from the article. What percentage is for the emotion and how it changes over time.






```{r}

```




